{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dN8xnbcqzfTK",
        "outputId": "2404d861-dcaf-4bd4-9c5b-a0adce02f01f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.2)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.1.0+cu121)\n",
            "Requirement already satisfied: accelerate>=0.20.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.27.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.10->transformers[torch]) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers[torch]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUGzbKUQhlpL",
        "outputId": "323b1bd9-50c8-4314-efeb-9f3833b5f503"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sumy in /usr/local/lib/python3.10/dist-packages (0.11.0)\n",
            "Requirement already satisfied: docopt<0.7,>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from sumy) (0.6.2)\n",
            "Requirement already satisfied: breadability>=0.1.20 in /usr/local/lib/python3.10/dist-packages (from sumy) (0.1.20)\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from sumy) (2.31.0)\n",
            "Requirement already satisfied: pycountry>=18.2.23 in /usr/local/lib/python3.10/dist-packages (from sumy) (23.12.11)\n",
            "Requirement already satisfied: nltk>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from sumy) (3.8.1)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from breadability>=0.1.20->sumy) (5.2.0)\n",
            "Requirement already satisfied: lxml>=2.0 in /usr/local/lib/python3.10/dist-packages (from breadability>=0.1.20->sumy) (4.9.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (4.66.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "pip install sumy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtfZJgRZhslQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ky_pSHWZQw48",
        "outputId": "202daccd-9964-4ed1-f293-c8c9d9c0672e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import BartForConditionalGeneration, BartTokenizer, AdamW\n",
        "import os\n",
        "\n",
        "import nltk\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
        "\n",
        "# Model\n",
        "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCW6yvsbRHdr",
        "outputId": "4dff5fc1-d863-4ddf-98dd-4952c9d23d67"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Optimizer and learning rate scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_text(text):\n",
        "    # Lowercasing\n",
        "    text = text.lower()\n",
        "    # Removing punctuation\n",
        "    text = re.sub(r'[^\\w\\s.]', '', text)\n",
        "    #      # removing digits\n",
        "    # text = re.sub(r'\\d', '', text)\n",
        "\n",
        "    # Removing stopwords\n",
        "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    text = ' '.join([token for token in tokens if token not in stop_words])\n",
        "    # Remove extra spaces\n",
        "    text = re.sub(' +', ' ', text)\n",
        "\n",
        "    # Remove extra spaces\n",
        "    text = re.sub(' +', ' ', text)\n",
        "    return text\n",
        "\n",
        "# # Specify the file paths for one specific pair of input and label\n",
        "# file_path_annual = '/content/drive/MyDrive/ProjectNLP/Annual_reports/17.txt'\n",
        "# file_path_summary = '/content/drive/MyDrive/ProjectNLP/gold_summaries/17_1.txt'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7vjaLZ_H3xL",
        "outputId": "270d3f3c-2e27-430a-e7e5-c1927483f507"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gizf5yGxrPcO",
        "outputId": "c25120a5-b328-4255-bf6e-ef4be5658d7d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BartForConditionalGeneration(\n",
              "  (model): BartModel(\n",
              "    (shared): Embedding(50264, 1024, padding_idx=1)\n",
              "    (encoder): BartEncoder(\n",
              "      (embed_tokens): Embedding(50264, 1024, padding_idx=1)\n",
              "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
              "      (layers): ModuleList(\n",
              "        (0-11): 12 x BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (decoder): BartDecoder(\n",
              "      (embed_tokens): Embedding(50264, 1024, padding_idx=1)\n",
              "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
              "      (layers): ModuleList(\n",
              "        (0-11): 12 x BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=1024, out_features=50264, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# trained_model.to(device)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfTRU0aqvfaY",
        "outputId": "1f44591d-ce40-4010-8db9-64ad0d9063fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated summary for 31076.txt\n",
            "Generated summary for 31114.txt\n"
          ]
        }
      ],
      "source": [
        "def summarize_report(report_path):\n",
        "    report_content = open(report_path, 'r', encoding='utf-8').read().strip()\n",
        "    report_content = preprocess_text(report_content)\n",
        "\n",
        "    sentences = nltk.tokenize.sent_tokenize(report_content)\n",
        "    length = 0\n",
        "    chunk = \"\"\n",
        "    chunks = []\n",
        "    count = -1\n",
        "\n",
        "    for sentence in sentences:\n",
        "        count += 1\n",
        "        combined_length = len(tokenizer.tokenize(sentence)) + length\n",
        "\n",
        "        if combined_length <= 1024:\n",
        "            chunk += sentence + \" \"\n",
        "            length = combined_length\n",
        "\n",
        "            if count == len(sentences) - 1:\n",
        "                chunks.append(chunk.strip())\n",
        "        else:\n",
        "            chunks.append(chunk.strip())\n",
        "            length = 0\n",
        "            chunk = \"\"\n",
        "            chunk += sentence + \" \"\n",
        "            length = len(tokenizer.tokenize(sentence))\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    # trained_model.to(device)\n",
        "    inputs = [tokenizer(chunk, return_tensors=\"pt\", max_length=1024, truncation=True, padding='max_length') for chunk in chunks]\n",
        "\n",
        "    chunk_summaries = []\n",
        "    for input in inputs:\n",
        "        # Ensure input tensor is on the same device as the model\n",
        "        input = {key: value.to(device) for key, value in input.items()}\n",
        "        output = model.generate(**input)\n",
        "\n",
        "        generated_summary = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "        chunk_summaries.append(generated_summary)\n",
        "\n",
        "    final_summary = ' '.join(chunk_summaries)\n",
        "    return final_summary\n",
        "\n",
        "# Path to the folder containing annual reports\n",
        "reports_folder = \"/content/drive/MyDrive/Rest_files\"\n",
        "# Path to the target folder to save generated summaries\n",
        "target_folder = \"/content/drive/MyDrive/Bart_summary\"\n",
        "index = 600\n",
        "\n",
        "# Iterate through all files in the reports folder\n",
        "for filename in os.listdir(reports_folder):\n",
        "    if filename.endswith(\".txt\"):\n",
        "        index += 1\n",
        "        report_path = os.path.join(reports_folder, filename)\n",
        "        generated_summary = summarize_report(report_path)\n",
        "\n",
        "        # Save the generated summary to the target folder with the same name as the input report\n",
        "        output_file_path = os.path.join(target_folder, f\"{filename.split('.')[0]}_{index}.txt\")\n",
        "        with open(output_file_path, \"w\", encoding=\"utf-8\") as output_file:\n",
        "            output_file.write(generated_summary)\n",
        "\n",
        "        print(f\"Generated summary for {filename}\")\n",
        "\n",
        "print(\"All reports summarized and saved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRQ60Vh8hBGN",
        "outputId": "dcfef235-e31f-4efd-c67a-8263ee71ab75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n",
            "summary Ganerated\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
        "\n",
        "def lexrank_summarize(text, num_sentences):\n",
        "    summarizer = LexRankSummarizer()\n",
        "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
        "    summary = summarizer(parser.document, num_sentences)\n",
        "    summary_sentences = [sentence._text for sentence in summary]\n",
        "    return ' '.join(summary_sentences)\n",
        "    # Combine sentences into a paragraph\n",
        "    summary_paragraph = ' '.join(summary_sentences)\n",
        "\n",
        "    return summary_paragraph\n",
        "\n",
        "\n",
        "def process_files(input_folder, output_folder, num_sentences=50):\n",
        "    # Ensure the output folder exists, create if not\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    # Process each .txt file in the input folder\n",
        "    for filename in os.listdir(input_folder):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            input_path = os.path.join(input_folder, filename)\n",
        "            output_path = os.path.join(output_folder, f\"{os.path.splitext(filename)[0]}.txt\")\n",
        "\n",
        "            with open(input_path, 'r', encoding='utf-8') as file:\n",
        "                input_text = file.read()\n",
        "            preprocces_input=preprocess_text(input_text)\n",
        "            # Apply LexRank summarization\n",
        "            summary_result = lexrank_summarize(preprocces_input, num_sentences)\n",
        "\n",
        "            # Save the summary to the output file\n",
        "            with open(output_path, 'w', encoding='utf-8') as output_file:\n",
        "                output_file.write(summary_result)\n",
        "                print (\"summary Ganerated {filename}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_folder_path = \"/content/drive/MyDrive/Bart_summary\"\n",
        "    output_folder_path = \"/content/drive/MyDrive/Bart_summary/Finale_summary\"\n",
        "    num_sentences_for_summary =50\n",
        "\n",
        "    process_files(input_folder_path, output_folder_path, num_sentences_for_summary)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from io import StringIO\n",
        "\n",
        "# Your CSV-like text data\n",
        "csv_data = \"/content/drive/MyDrive/Result_Bart_lexrank_main.csv\"\n",
        "\n",
        "# Create a pandas DataFrame from the CSV-like text\n",
        "df = pd.read_csv(csv_data)\n",
        "\n",
        "# Group by 'ROUGE-Type' and calculate the average for each ROUGE type\n",
        "averages = df.groupby('ROUGE-Type').mean().reset_index()\n",
        "\n",
        "# Print the resulting DataFrame\n",
        "print(averages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUbd0sJUDaJO",
        "outputId": "b7d57436-52d2-4f25-d5cf-8e32bcf4a850"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                           ROUGE-Type     Task Name  Avg_Recall  \\\n",
            "0    ROUGE-1+StopWordRemoval+Stemming  30910.759036    0.323981   \n",
            "1    ROUGE-2+StopWordRemoval+Stemming  30910.759036    0.109059   \n",
            "2    ROUGE-L+StopWordRemoval+Stemming  30910.759036    0.327257   \n",
            "3  ROUGE-SU4+StopWordRemoval+Stemming  30910.759036    0.168824   \n",
            "\n",
            "   Avg_Precision  Avg_F-Score  Num Reference Summaries  \n",
            "0       0.203830     0.244204                 3.445783  \n",
            "1       0.049646     0.065568                 3.445783  \n",
            "2       0.256650     0.283103                 3.445783  \n",
            "3       0.069800     0.095886                 3.445783  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-0f437ba2c2c5>:11: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
            "  averages = df.groupby('ROUGE-Type').mean().reset_index()\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}